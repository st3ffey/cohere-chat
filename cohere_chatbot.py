# -*- coding: utf-8 -*-
"""Cohere Chatbot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q2K1833G1x8MFY4v_VCGq0IPaJR7J57x

## **Installs, configs**
"""

!pip install cohere PyPDF2 faiss-cpu gradio

# importing necessary inputs
from google.colab import userdata # for secrets
import cohere # sota llm ops
import PyPDF2 # extracting pdf text
import faiss # open source local vector store
import numpy as np # numpy :)

# initializing cohere client
coherekey = userdata.get('coherekey')
co = cohere.Client(coherekey)

"""## **Setup, embedding**"""

from PyPDF2 import PdfReader

# load pdf
pdf_path = "/content/sonyspeech-unlocked.pdf"
reader = PdfReader(pdf_path)

# setting standard chunk size
chunk_size = 1000

chunks = []
current_chunk = ""
page_num = 1

# extract text from each page in the pdf
for page in reader.pages:
    text = page.extract_text()

    # chunking
    for char in text:
        current_chunk += char

        if len(current_chunk) >= chunk_size:
            # add page number metadata
            chunk_with_metadata = f"[Page {page_num}] {current_chunk}"
            chunks.append(chunk_with_metadata)
            current_chunk = ""

    page_num += 1

# grab the last chunk, if any
if current_chunk:
    chunk_with_metadata = f"[Page {page_num-1}] {current_chunk}"
    chunks.append(chunk_with_metadata)

embeddings = []

# create embeddings from the chunked pdf
for chunk in chunks:
    response = co.embed(texts=[chunk], model='embed-english-v3.0', input_type="search_document")
    embedding = response.embeddings[0] # list of lists
    embeddings.append(embedding)

# creating the vector store with proper dimensions
d = len(embeddings[0])
index = faiss.IndexFlatL2(d)

# add the embeddings
embeddings_np = np.array(embeddings, dtype=np.float32)
index.add(embeddings_np)

# write the index, for later reference
faiss.write_index(index, 'chunk_embeddings.index')

"""## **Pipeline**"""

# load the index
index = faiss.read_index('chunk_embeddings.index')

# function to retrieve similar chunks
def retrieve_similar_chunks(query, k=10):
    query_embedding = co.embed(texts=[query], model='embed-english-v3.0', input_type="search_query").embeddings[0]
    distances, indices = index.search(np.array([query_embedding], dtype=np.float32), k)
    similar_chunks = [chunks[i] for i in indices[0]]
    return similar_chunks

# function to rerank the retrieved chunks, to ensure maximum relevancy
def rerank_chunks(query, chunks):
    reranker_response = co.rerank(
        model='rerank-english-v3.0',
        query=query,
        top_n=6,
        documents=chunks,
    )
    reranked_candidates = reranker_response.results
    doc_content = [chunks[candidate.index] for candidate in reranked_candidates]
    return doc_content

# function to generate a response with the chat endpoint
def generate_response(prompt, history):
    response = co.chat(
        model='command-r',
        preamble="You are a nice AI assistant that has access to documents containing information that users want to know about. \
        When you answer, include the each page number from the chunk where you found the answer. The user needs the \
        page number associated with the information. If the user doesn't ask a question relevant to any information, just answer normally.",
        message=f"Use the following query and context to answer the user's question. Here: {prompt}.",
        max_tokens=400,
        temperature=0.1,
        chat_history=history
    )
    return response.text.strip()

# main pipeline
def rag_pipeline(query):
    chat_history.append({'role': 'USER', 'message': query})

    # prepare the prompt
    prompt = f"User query: {query}\n\nRelevant information:\n"
    similar_chunks = retrieve_similar_chunks(query)
    reranked_chunks = rerank_chunks(query, similar_chunks)

    for chunk in reranked_chunks:
        prompt += f"- {chunk}\n"

    prompt += "\nUser prompt:"

    # generate the response with history
    response = generate_response(prompt, chat_history)

    # update history with the bot's response
    chat_history.append({'role': 'CHATBOT', 'message': response})

    return response

"""## **Chat**"""

import gradio as gr

# maintaining a history list
chat_history = []

# to reset the chat history
def reset_history():
    global chat_history
    chat_history = []
    return "History has been reset!"

# gradio for a nice interface :)
def main():
    with gr.Blocks() as interface:
        gr.Markdown("## Chat with Sony's Q3 2023 Financials!")
        gr.Markdown("Enter your question about Sony's Q3 performance, and receive a response from the chatbot using Cohere reranked retrieval.")
        input_text = gr.Textbox(label="Question:")
        output_text = gr.Textbox(label="Answer:")
        reset_btn = gr.Button("Reset History")
        reset_btn.click(fn=reset_history, inputs=[], outputs=[output_text])
        submit_btn = gr.Button("Submit")
        submit_btn.click(fn=rag_pipeline, inputs=[input_text], outputs=[output_text])

    interface.launch(debug=True)

if __name__ == "__main__":
    main()